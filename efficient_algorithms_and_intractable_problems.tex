\documentclass[11pt]{article}

\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate,amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{threeparttable, adjustbox, booktabs}
\usepackage{outlines}
\usepackage{multicol}

\def\endproofmark{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\renewcommand\arraystretch{1.5}

%-----------------------------------------------------------------------------------

% Title information
\title{Efficient Algorithms and Intractable Problems}
\author{Daniel Deng}
\pagestyle{myheadings}
\date{}

%-----------------------------------------------------------------------------------

\begin{document}
\maketitle

\section{Complexity Analysis}
\subsection{Partial Sums}
\begin{align*}
S_k = \sum_{n=1}^k a_n = \frac{k}{2}(a_1+a_k) && \text{(Arithmetic Series)} \\
S_k = \sum_{n=1}^k a_1(r)^n = a_1 \left(\frac{1-r^k}{1-r}\right) && \text{(Geometric Series)} \\
\end{align*}

\subsection{Asymptotic Relations}
\begin{align*}
f = O(g) \approx f(n) \leq c\cdot g(n) \\
f = o(g) \approx f(n) < c\cdot g(n) \\
f = \Omega(g) \approx f(n) \geq c\cdot g(n) \\
f = \omega(g) \approx f(n) > c\cdot g(n) \\
f = \Theta(g) \approx f(n) = c\cdot g(n) \\
\end{align*}

\begin{remark}
$O(i^n \mid i > 1) > O(n^j) > O(log^k n)$
\end{remark}

\begin{theorem}[Master Theorem]
If $T(n)=aT([n/b])+O(n^d)$ for some constants $a>0$, $b>1$, and $d\geq 0$, then
\begin{equation*}
T(n)=\begin{cases}
\Theta(n^d) &a<b^d \\
\Theta(n^d \log n) & a = b^d \\
\Theta(n^{\log_b a}) & a > b^d \\
\end{cases}
\end{equation*}
\end{theorem}
\clearpage

\section{Polynomial Interpolation}
Given a degree $n$ polynomial $A(x)=a_0 + a_1 x + a_2 x^2+\cdots + a_{n} x^n$, the relationship between its values and coefficients can be represented by
$$
\begin{bmatrix}
A(x_0) \\ A(x_1) \\ \vdots \\ A(x_{n-1}) \\
\end{bmatrix} = \begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^{n-1} \\
1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
&& \vdots \\
1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-1} \\
\end{bmatrix} \begin{bmatrix}
a_0 \\ a_1 \\ \vdots \\ a_{n-1} \\
\end{bmatrix} \quad (\mbox{evaluation})
$$
where the matrix $M$ is a \textit{Vandermonde} matrix.

\subsection{Fast Fourier Transform (FFT)}
\begin{definition}[Discrete Fourier Transform Matrix]
For polynomials of degree $<n$ ($n$ is even; polynomials can be 0-padded), the Discrete Fourier Transform can be represented by the matrix
$$M_n (\omega) = \begin{bmatrix}
1 & 1 & 1 & \cdots & 1 \\
1 & \omega & \omega^2 & \cdots & \omega^{n-1} \\
&&\vdots \\
1 & \omega^j & \omega^{2j} & \cdots & \omega^{(n-1)j} \\
&&\vdots \\
1 & \omega^{n-1} & \omega^{2(n-1)} & \cdots & \omega^{(n-1)(n-1)} \\
\end{bmatrix}$$ \\
where $\omega = e^{2\pi i / n}$ is the $n$th root of unity, and $M_n (\omega)$ is an unitary matrix whose columns forms the \textit{Fourier Basis}.
\end{definition}
\begin{remark}
$M_n^{-1} (\omega) = \frac{1}{n} \overline{M_n (\omega)} = \frac{1}{n} M_n (\omega^{-1})$
\end{remark}
\clearpage

\begin{algorithm}[ht]
    \caption{Fast Fourier transform}
	\SetKwFunction{FFT}{FFT}  
	\SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
	\KwIn{A coefficient vector, $\vec{a}=\langle a_0,\dots,a_{n-1} \rangle$ and the $n$th root of unity, $\omega$.}
	\KwOut{$M_n(\omega)\vec{a}$}
    
    \Fn{\FFT{$\vec{a}$, $\omega$}}{
    \eIf{$\omega = 1$}{
    	return $\vec{a}$
    } {
    \tcp{$A(x)=A_{even}(x^2)+xA_{odd}(x^2)$}
    	$\langle A_e(0),\dots,A_{e}(n/2-1)\rangle \gets \mbox{FFT}(\langle a_0, a_2,\dots,a_{n-2}\rangle,\omega^2)$ \\
    	$\langle A_o(0),\dots,A_{o}(n/2-1)\rangle \gets \mbox{FFT}(\langle a_1, a_3,\dots,a_{n-1}\rangle,\omega^2)$ \\
    	\For {$j:=0$ to $n/2-1$}{
    		$A(j) \gets A_{e}(j) + \omega^j A_{o}(j)$ \\
    		$A(j+n/2) \gets A_{e}(j) - \omega^j A_o(j)$ \\
    	}
    }
    \Return {$\langle A(0), \dots, A(n-1)\rangle$}}
\end{algorithm}
\begin{remark}
If $A$ is evaluated at points $\pm \omega_0,\dots, \pm \omega_{n/2-1}$, then $A_e(x^2)$ and $A_o(x^2)$ will only need to evaluate half the amount of points due to squaring.
$$T(n) = 2T(n/2) + O(n) = O(n \log n)$$
\end{remark}

\subsection{Applications of FFT}
\begin{algorithm}[ht]
    \caption{Fast Polynomial Multiplication}
    
    \DontPrintSemicolon
    \KwIn{Coefficient vectors, $a$ and $b$, and the $n$th root of unity, $\omega$.}
    \KwOut{The coefficient vector of $A(x)B(x)$}
    
    $\hat{\vec{a}} \gets M_n(\omega)\vec{a}$ \quad \mbox{(FFT)}\\
    $\hat{\vec{b}} \gets M_n(\omega)\vec{b}$\\
    \For {$i=0$ to $n-1$}{$\hat{c}_i\gets \hat{a}_i\hat{b}_i$}
    \Return {$\frac{1}{n} M_n(\omega^{-1})\hat{\vec{c}}$} \quad \mbox{(inverse matrix)}
\end{algorithm}

\begin{definition}[Cross-Correlation]
$corr(\vec{x},\vec{y})[k] = \sum x_i y_{i-k}$, which measures similarity.
\end{definition}

\begin{algorithm}
    \caption{Cross-Correlation}
 
    \DontPrintSemicolon
    \KwIn{Two signal vectors, $\vec{x}$ and $\vec{y}$.}
    \KwOut{$corr(\vec{x},\vec{y})$}
    
    $X(t) \gets x_{m-1} + x_{m-2}t + \dots + x_0 t^{m-1}$ \\
    $Y(t) \gets y_0 + y_1 t + \dots + y_{n-1} t^{n-1}$ \\
    $Q(t) \gets X(t)Y(t)$ \quad \mbox{(Fast Polynomial Multiplication)}\\
    \Return {$\vec{q}$}
\end{algorithm}
\clearpage

\section{Graphs}
\begin{definition}[Graph] A graph is a pair $G=(V,E)$, typically represented by an adjacency matrix or an adjacency list.
\end{definition}

\begin{table}[ht]
\centering
\caption{Graph representations.}
\begin{tabular}[t]{lcccc}
\hline
& Space & Connectivity & getNeighbors($u$) & DFS Runtime\\
\hline
Adjacency Matrix & $\Theta(|V|^2)$ & $O(1)$ & $\Theta(|V|)$ & $\Theta(|V|^2)$\\
Adjacency List & $\Theta(|V|+|E|)$ & $\Theta(degree(u))$ & $\Theta(degree(u))$ & $\Theta(|V|+|E|)$ \\
\hline
\end{tabular}
\end{table}

\subsection{Depth-First Search}
\begin{algorithm}[ht]
    \caption{Depth-first search}
    \SetKwFunction{DFS}{DFS}
    \SetKwFunction{EXPLORE}{EXPLORE}
    \SetKwProg{Fn}{Function}{:}{}  
 
    \DontPrintSemicolon
    
	\KwIn{$V, E$ of directed graph $G$.}   
    
	\Fn{\DFS{$V,E$}}{
		$n \gets |V|$ \\
		$clk \gets 1$ \\
    	visited $\gets$ boolean[$n$] \\
    	preorder, postorder = int[$n$] \\
    	\For {$v \in V$} {
    		\If {!visited[v]} {
    			\EXPLORE{v}{}
    		}
    	}
	}
	\Fn{\EXPLORE{$v$}} {
		visited[v] $\gets$ True \\
		preorder[v] $\gets$ clk++ \\
		\For {$(v,w)\in E$}{
			\If{!visited[w]} {
				\EXPLORE{w}
			}		
		}
		postorder[v] $\gets$ clk++ \\
	}
	\tcc{Preorder-postorder intervals are either nested or disjoint.
}
	\tcc{postorder[$u$] $\leq$ postorder[$v$] \textit{iff} $(u,v)$ is a back edge.
}
	\tcc{$G$ contains a cycle \textit{iff} it contains a back edge.
}
\end{algorithm}

\subsubsection{Applications of DFS}
\begin{algorithm}[ht]
    \caption{Topological sort.}
    
    \DontPrintSemicolon
    \KwIn{A directed cyclic graph $G$.}
    \KwOut{An ordered list of $V$ such that $u_i$ comes before $v_i$ for all $(u_i, v_i)\in E$ (i.e., ordered by decreasing dependency).}
    
    $post \gets$ DFS-visited vertexes ordered by postorder visits \\
    \Return{reverse(post)}
\end{algorithm}

\begin{definition}[Strongly Connected Component]
A SCC is a maximal partition of a directed graph in which every vertex is reachable from every other vertex.
\end{definition}
\begin{align*}
u \text{ is in sink SCC of graph }G &\Leftrightarrow u \text{ is in source SCC of reverse graph }G \\ &\Leftrightarrow 
\text{u is in source SCC if highest postorder number.}
\end{align*}


\subsection{Single-Source Shortest Path}
\begin{algorithm}[ht]
    \caption{Single-Source Shortest Path}    
    
	\SetKwFunction{BFS}{BFS} 
	\SetKwFunction{Dijkstra}{Dijkstra} 
	\SetKwFunction{BF}{Bellman-Ford}
    \SetKwProg{Fn}{Function}{:}{}      
    
    \KwIn{A directed graph $G$ and a start vertex $S$.}
    \KwOut{Two arrays $prev[|V|]$ (shortest-path predecessor) and $dist[|V|]$ (shortest-path distance).}
    
    \Fn{\BFS{$G,S$}}{
	\tcc{Must have uniform edge weights. $O(|V|+|E|)$ runtime.}    
    }
    
    \Fn{\Dijkstra($G, S$)}{
    \tcc{Must have positive edge weights. $O(|V| \log |V| + |E|)$ runtime if implemented using Fibonacci heap.}
    }
    
    \Fn{\BF($G,S$)}{
    \tcc{Can have aribitrary edge weights.}
    }
\end{algorithm}
\clearpage

\subsection{Minimum Spanning Tree}
\begin{algorithm}[ht]
    \caption{Minimum spanning tree.}
    \SetKwFunction{P}{Prim}
    \SetKwFunction{K}{Kruskal}
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    \KwIn{A graph $G$ and a starting vertex $v$.}
    \KwOut{The minimum spanning tree $T$ of $G$.}
    \tcc{Use the cut property.}
    
    \Fn{\P{$G,v$}}{
    	\tcc{Sequentially adds the closest neightbor of the running set. $O(|E| + \log|V|)$ runtime if implemented using Fibonacci heap.}
    }
    
    \Fn{\K{$G,v$}}{
    	\tcc{Sequentially adds the shortest edge that does not create a cycle. $O(|E|\log|V|)$ runtime if implemented using Union-Find data structure.}
    }
\end{algorithm}
\clearpage

\section{Greedy Algorithm}
\begin{definition}[Greedy Algorithm]
A greedy algorithm is one that builds the solution iteratively using a sequence of local choices.
\end{definition}

\begin{algorithm}
    \caption{Example greedy algorithms.}
    \SetKwFunction{Scheduling}{SCHEDULING}  
    \SetKwFunction{Huffman}{HUFFMAN}
    \SetKwFunction{Set}{SET-COVER}
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    
    \Fn{\Scheduling} {
	\tcc{Find the maximum set of jobs that can be completed within time by iteratively select the next job to have the smallest end time without conflicting existing schedule. $O(n \log n)$ runtime if sorting the collection of jobs first.}    
    }
    
    \Fn{\Huffman}{
    \tcc{Find a prefix tree for prefix-free Huffman coding by iteratively combine the two least frequent elements of the alphabet and retrieve the order of the prefix tree accordingly. $O(n \log n)$ runtime if implemented with min-heap.}
    }
    
    \KwIn{A set of partitions $S=\{S_1,\dots,S_m\}$ that covers the universe $\{1,\dots,n\}$.}
    \KwOut{The indices of the smallest sub-collection of $S$ that covers the universe.}
	\Fn{\Set}{	
	\tcc{Greedy search yields sub-optimal but competitive solution to the set-cover problem. If the optimal solution uses $k$ sets, then the greedy solution uses at most $k \ln n$ sets.}    
    $A \gets \{1,\dots,n\}$ \\
    $B \gets \emptyset$ \\
    \While{$|A|>0$}{
    	let $i\in [m]\setminus B$ be s.t. $|A \cap S_i|$ is maximum \\
    	$A \gets A \setminus S_i$ \\
    	$B \gets B \cup {i}$
    }   \Return{$B$}    }
\end{algorithm}
\clearpage

\section{Union-Find}
\begin{definition}[Amortized Analysis]
Suppose a data structure supports $k$ operations. Then the amortized cost of each operation is $t_i$ if for any sequence of operations with $N_i$ of $O_i$ operations, the total time is at most $\sum_{i=1}^k t_i N_i$.
\end{definition}

\begin{algorithm}
    \caption{Union-find (disjoint forest implementation).}
    \SetKwFunction{Make}{MAKE-SET}
    \SetKwFunction{Find}{FIND} 
    \SetKwFunction{Union}{UNION} 
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    
	\tcc{$O((m+n)\log^* n)$ runtime.}    
    
    $parent[1,\dots,n]$\\
    $rank[1,\dots,n]$     \tcp{rank is defined as the height if no path compression occur.}
    \Fn{\Make{$x$}}{
	$parent[x] \gets x$ \\
	$rank[x] \gets 0$ \\    
    }
    
    \Fn{\Find{$x$}} {
	\If{$x = parent[x]$} {
	\Return{$x$}
	}
	$parent[x] \gets$ \Find{$parent[x]$} \tcp{path compression}
    \Return{$parent[x]$}
    }
    
    \Fn{\Union{$x,y$}} {
    	$x\gets$ \Find{$x$} \\
    	$y\gets$ \Find{$y$} \\
    	\If{$x=y$} {
    		\Return \tcp{no work needed}
    	}
    	\If{$rank[x]>rank[y]$}{
			swap $x$ and $y$    	
    	}
    	$parent[x] \gets y$
    	\If{$rank[x] = rank[y]$} {
			$rank[y] \gets rank[y] + 1$    	
    	}
    }
\end{algorithm}
\begin{remark}
Union-Find Invariants:
\begin{itemize}
\item a tree rooted at $x$ has $\geq 2^{r[x]}$ items;
\item $(\forall x)$, if $x$ is not a root, $r[p[x]] > r[x]$;
\item the number of items of exactly rank $k$ is $\leq \frac{n}{2^k}$.
\end{itemize}
\end{remark}

\section{Dynamic Programming}
\begin{definition}[Top-Down DP/Memoization]
Recursion + look-up table
\end{definition}

\begin{definition}[Bottom-Up DP]
Fill up the look-up table iteratively instead of recursively
\end{definition}

\begin{remark}
Bottom-up DP sometimes have better memory
\end{remark}

\subsection{Bellman-Ford}

\begin{definition}[Bellman-Ford]
An algorithm for finding the SSSP on a directed graph with potentially negative weights. The algorithm defines a function $f(t,k) :=$ the length of the shortest path from $s$ to $t$ using $\leq k$ edges, and wishes to solve for $f(t,n-1)$. The recurrence relation of the algorithm is
\[
f(t,k) = \begin{cases}
\infty & k=0,t\neq s\\
0 & k=0, t = s \\
\min \begin{cases}
f(t,k-1) \\
\min_{(v,t)\in E} w(v,t)+f(v,k-1) \\
\end{cases} & else
\end{cases}
\]
\end{definition}

\begin{remark}
Negative cycle detection: $\exists$ negative cycle $\Leftrightarrow$ $\exists v, f(v,n)<f(v,n-1)$
\end{remark}

\begin{algorithm}
    \caption{Bellman-Ford}
    \SetKwFunction{FFT}{FFT}  
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    \KwIn{$G, s$.}
    
    initialize $T[1,\dots,n]$ to all $\infty$ \\
    $T[s] \gets 0$ \\
    \For{$k = 1$ to $n-1$} {
    	\ForEach{$(u,v)\in E$} {
    		$T[v] \gets \min \begin{cases}
    			T[v]\\
    			w(u,v) + T(u) \\
    		\end{cases}
    		$
    	}
    } \Return{$T$}
\end{algorithm}

\begin{itemize}
\item Memory: $O(n)$ with bottom-up
\item Runtime: $O(n^2 +mn)$
\end{itemize}


\subsection{Floyd-Warshall}
\begin{definition}[Floyd-Warshall]
An algorithm for finding the all pairs shortest path (APSP) on a directed graph. Assume the graph is complete (pretend $w(e)=\infty$ for all $e \not\in E$), the algorithm defines a function $f(i,j,k) :=$ the length of the shortest path from $i$ to $j$ when all intermediate vertices in the path must be in $\{1,\dots,k\}$.  The recurrence relation is
\[
f(i,j,k) = \begin{cases}
w(i,j) & k=0, i \neq j \\
0 & k=0, i = j \\
\min\begin{cases}
f(i,j,k-1) \\
f(i,k,k-1) + f(k,j,k-1) \\
\end{cases} & else
\end{cases}
\]
\end{definition}

\begin{algorithm}
    \caption{Floyd-Warshall}
    \SetKwProg{Fn}{Function}{:}{}
    %\SetKwFunction{•}{•}  
    
    \DontPrintSemicolon
    \KwIn{G}
    %\KwOut{•}
    
    initialize $T[1\dots n][1\dots n]$ such that $T[i][j]\gets w(i,j)$ for all $i,j$ \\
    \For{$k=1$ to $n$} {
    	\For{$i = 1$ to $n$} {
    		\For{$j = 1$ to $n$} {
    			$T[i][j] \gets \begin{cases}
    			T[i][j] \\
    			T[i][k] + T[k][j] \\
    			\end{cases}
    			$
    		}
    	}
    } \Return{$T$}
\end{algorithm}

\begin{itemize}
\item Memory: $O(n^2)$
\item Runtime: $O(n^3)$
\end{itemize}

\subsection{Longest Increasing Subsection}
\begin{definition}[LIS]
Define $f(last, i) :=$ length of the LIS of $A[i \dots n]$ such that all values used are $> A[last]$. The recurrence relation is
\[
f(last, i) = \begin{cases}
0 & i = n+1 \\
f(last, i+1) & A[i] \leq A[last] \\
\max \begin{cases}
f(last, i+1) \\
f(i, i+1) + 1 \\
\end{cases}
\end{cases}
\]
\end{definition}

\begin{itemize}
\item Memory: $O(n)$
\item Runtime: $O(n^2)$
\end{itemize}

\subsection{Knapsack}
\begin{definition}[Knapsack]
Given an array $A[1\dots n]$ of items, each being a (weight, value) pair. Given a knapsack that can hold $\leq W$ weight, find the maximum value containable in the knapsack. The algorithm for solving this problem defines a function $f(i, C) := $ maximum value we can pack among $A[i\dots n]$ with capacity $C$. The recurrence relation is
\[
f(i,C) = \begin{cases}
0 & i = n+1 \\
f(i+1, C) & w[i] > C \\
\max \begin{cases}
f(i+1, C) \\
f(i+1, C - w[i]) + v[i] \\
\end{cases} & else
\end{cases}
\]
\end{definition}

\begin{itemize}
\item Memory: $O(W)$
\item Runtime: $O(nW)$ \qquad *pseudopolynomial
\end{itemize}

\begin{remark}
For knapsack problems with replacement, the recurrence relation is instead
\[
f(C) = \max_{i} \left(f(C - w[i]) + v[i]\right)
\]
\end{remark}

\subsection{Traveling Salesman Problem}
\begin{definition}[Traveling Salesman Problem]
Given $n$ locations with distances $D[i][j]$. The traveling salesman wishes to visit all locations, starting at $1$, while minimizing total travel distance. The DP algorithm defines a function $f(i,S) :=$ minimum traveling distance to visit all locations in $S$ when starting at $1$. The recurrence relation is
\[
f(i,S) = \begin{cases}
0 & S\neq \emptyset \\
\min_{x\in S} \left(D[i][x]+f\left(x, S \setminus \{x\}\right)\right) & else
\end{cases}
\]
\end{definition}

\begin{itemize}
\item Memory: $O(\sqrt{n} \cdot 2^n)$
\item Runtime: $O(n^2 \cdot 2^n)$
\end{itemize}


\subsection{Matrix Chain Multiplication}
\begin{definition}[Matrix Chain Multiplication]
Given $s_1,\dots,s_{n+1}$ such that $A_i$ is a $s_i \times s_{i+1}$ matrix, and we want to find the minimum number of flops to compute $A_1 \times \dots \times A_n$. The DP algorithm defines a function $f(i,j):=$ minimum number of flops to compute $A_i \times \dots \times A_j$. The recurrence relation is
\[
f(i,j) = \begin{cases}
0 & i = j \\
\min_{i\leq k\leq j} \left(f(i,k) + f(k+1, j) + s_i s_{j+i} s_{k+1}  \right) & else \\
\end{cases}
\]
\end{definition}

\begin{itemize}
\item Memory: $O(n^2)$
\item Runtime: $O(n^3)$
\end{itemize}

\clearpage

\section{Linear Programming}
\begin{definition}[Linear Programming] Linear programming (LP) describes a broad class of optimization tasks in which both the constraints and the optimization criterion are linear functions. The optimum of a linear program is achieved at a vertex of the convex feasible region.
\end{definition}

\begin{remark}
A linear program does not have an optimum \textit{iff} its feasible region is infeasible and/or unbounded.
\end{remark}

\begin{definition}[Simplex Method]
A standard greedy algorithm for solving LP by hill-climbing on vertices of the feasible region.
\end{definition}
\begin{remark}
Solves real-life LP in polynomial time.
\end{remark}

\subsection{LP Conversion}
\begin{outline}[enumerate]
\1 (Maximization $\leftrightarrow$ minimization) multiply the
coefficients of the objective function by $-1$
\1 (Inequality $\to$ equality) $ax\leq b \to ax+s = b \mid s \geq 0$
\1 (Equality $\to$ Inequality) $ax=b \to ax\leq b \wedge ax\geq b$
\1 (Signed $\leftrightarrow$ unsigned) $x \leftrightarrow x^+ - x^- \mid x^+,x^- \geq 0$
\end{outline}

\subsection{Duality}
\begin{theorem}[Duality theorem]
If a linear program has a bounded optimum, then so does its dual, and the
two optimum values coincide (strong duality; weak duality states that primal opt. $\leq$ dual opt.).
\end{theorem}

\noindent
\begin{minipage}{0.49\textwidth}
Primal LP:
\begin{align*}
\max c^\textsf{T} x\\
Ax \leq b  \\
b \geq 0 \\
\end{align*}
\end{minipage}
\noindent
\begin{minipage}{0.49\textwidth}
Dual LP:
\begin{align*}
\min y^\textsc{T} b\\
y^\textsc{T} A \geq c^\textsc{T} \\
y \geq 0 \\
\end{align*}
\end{minipage}
\begin{remark}
Dual/Primal unbounded $\implies$ Primal/Dual unfeasible.
\end{remark}

\subsection{Network Flow}
\begin{definition}[Flow] Given a directed graph $G=(V,E)$ with capacities $c_e>0$ on all edges. The flow $f$ from source $s$ to sink $t$ satisfies the constraints:
\begin{outline}[enumerate]
\1 For all $e\in E$, 
$$0\leq f_e\leq c_e$$ 
\1 For all $e\in E\setminus\{s,t\}$,
$$\sum_{(w,u)\in E} f_{wu} = \sum_{(u,z)\in E} f_{uz} \quad \text{(conservation of flow)}$$
\end{outline}
\end{definition}

\begin{remark} By the conservation principle,
\[\text{size}(f) = \sum_{(s,u)\in E} f_{su}\]
\end{remark}

\begin{algorithm}[ht]
    \caption{Ford-Fulkerson}
    \tcp{Simplex algorithm for solving max-flow problem. Pseudopolynomial time complexity; $O(|E|F)$}
    \DontPrintSemicolon
    \KwIn{$G=(V,E),s,t$}
    \KwOut{$f$}
    
    \tcp{$G_f :=$ residual graph (also contains back-edges)}
    \While{$\exists$ an augmenting path in $G_f$}{
    	Find an arbitrary augmenting path $P$ from $s$ to $t$ \\
    	Augment flow $f$ along $P$ \\
    	Update $G_f$ \\
    }
    
\end{algorithm}

\begin{theorem}[Max-flow Min-cut Theorem]
The size of the maximum flow in a network equals the capacity
of the smallest $(s,t)$-cut $(L,R)$ (total capacity of the edges crossing the cut)
\end{theorem}

\begin{remark}
$L$ contains all reachable vertices from $s$ in the final residual $G^f$ and $R$ contains all remaining vertices.
\end{remark}

\subsection{Zero-Sum Game}
\begin{theorem}[Min-Max Theorem] For zero-sum games, there exists an equilibrium such that
\[\underset{x}{\max}\, \underset{y}{\min}\, \sum_{i,j}G_{i,j}x_i y_j = \underset{y}{\min}\,\underset{x}{\max}\,  \sum_{i,j}G_{i,j}x_i y_j\]
\end{theorem}
\begin{remark}
To convert game strategies to LP:
\begin{itemize}
\item $\underset{x}{\max}\, \underset{y}{\min}\, \sum_{i,j}G_{i,j}x_i y_j \implies$
\begin{align*}
\max z \\
\forall y, \sum_{i,j}G_{i,j}x_i y_j \geq z \\
\sum_i x_i = 1 \\
\forall i, x_i \geq 0 \\
\end{align*}

\item $\underset{y}{\min}\,\underset{x}{\max}\,  \sum_{i,j}G_{i,j}x_i y_j \implies$
\begin{align*}
\min w \\
\forall x, \sum_{i,j}G_{i,j}x_i y_j \leq w \\
\sum_j y_j = 1 \\
\forall j, y_j \geq 0 \\
\end{align*}
\end{itemize}
It is apparent that the two LPs are dual; therefore, the equilibrium can be found in polynomial time via LP.
\end{remark}
\clearpage

\section{Multiplicative Weight Updates}
\begin{definition}[Online Decision Making]
A problem where one chooses to follow expert $i^{(t)}$ out of $n$ experts on day $t \in \{1,\dots,T\}$, who incurs a loss of $l_i^{(t)}$ on day $t$ ($\forall i, t, l_i^{(t)}$ is bounded by $[0,1]$; range can be normalized), with the goal of minimizing the total loss 
\[L := \sum_{t=1}^T l_i ^{(t)}\]
Realistically, the problem aims to minimize the regret
\[R := L - L^* \qquad \left(  L^* := \min_{i\in [n]} \sum_{t=1}^T l_i^{(t)}    \right)\]
\end{definition}

\begin{remark}
It would be trivial to define the offline optimum $L^*$ as $\sum_{t=1}^T \min_{i \in [n]} l_i^{(t)}$ in minimizing regret.
\end{remark}

\subsection{Hedge/MWU}
\begin{algorithm}
    \caption{Hedge/MWU}
    
	\tcc{Defined expected loss on day $t$ to be $L_t := \left<x^{(t)}, l^{(t)}\right>$ and the total loss to be $L := \sum_{t=1}^{T} L_t$, where $x^{(t)}$ is the probability distribution of choosing any expert on day $t$.}  
    
    \DontPrintSemicolon
    \KwIn{$\epsilon \in [0, \frac{1}{2}]$.}
    
    $\forall i \in [n], w_i^{(1)} \gets 1$ \\
    $x_i^{(t)} \gets \frac{w_i^{(t)}}{W^{(t)}} \qquad$ \tcp{$\left( W^{(t)} := \sum_{j=1}^{n} w_j^{(t)} \right)$}
    $w_i^{(t+1)} \gets w_i^{(t)} (1-\epsilon)^{l_i^{(t)}}$
\end{algorithm}

\begin{lemma}
$W^{(T+1)} \geq (1-\epsilon)^{L^*}$
\end{lemma}

\begin{proof}
\begin{align*}
W^{(T+1)} &\geq w_{i^*}^{(T+1)} \\
& = \prod_{t+1}^T (1-\epsilon)^{l_{i^*}^{(t)}} \\
& = (1-\epsilon)^{L^*}
\end{align*}
\end{proof}

\begin{lemma}
$W^{(T+1)} \leq n \cdot \prod_{t=1}^{T} (1 - \epsilon \cdot L_t)$
\end{lemma}

\begin{proof}
TODO
\end{proof}

\begin{theorem}
Hedge$(\epsilon)$ achieves $\mathbb{E}[R] \leq \epsilon \cdot T + \frac{\ln n}{\epsilon}$ (or $\mathbb{E}[R] \leq 2 \sqrt{T \ln n}$ if $\epsilon = \sqrt{\frac{\ln n}{T}}$).
\end{theorem}

\begin{proof}
\begin{align*}
& (1-\epsilon)^{L^*} \leq n \cdot \prod_{t=1}^{T} (1 - \epsilon \cdot L_t) \\ 
\implies & L^* \ln (1-\epsilon) \leq \ln n + \sum _{t+1}^n \ln (1 - \epsilon \cdot L_t) \\
\implies & L^* (-\epsilon-\epsilon^2) \leq \ln n - \epsilon \sum_{t=1}^T L_t \\
\implies & \sum_{t=1}^T L_t - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \cdot L^* \\
\implies & \mathbb{E}[R] \leq \epsilon \cdot T + \frac{\ln n}{\epsilon}
\end{align*}
\end{proof}

\begin{remark} $\forall z \in [0,\frac{1}{2}], -z-z^2 \leq \ln (1-z) \leq -z$
\end{remark}
\clearpage

\section{Reductions}
\begin{definition}[Reduction]
Given two problems $A$ and $B$. If $A$ reduces to $B$, $\exists$ efficient algorithm for $B \implies \exists$ efficient algorithm for $A$.
\end{definition}

\clearpage

\section{Search Problems}
\begin{definition}[Binary relations]
\begin{align*}
(x,w) \in R \subseteq \{ 0,1 \}^* \times \{ 0,1 \}^*
\end{align*}
\end{definition}

\section{P/NP}
\begin{definition}[P (polynomial-time)] P  is the class of all $R$ such that there exists an algorithm for solving $Decide(R)$ in time $poly(|x|)$.

\begin{definition}[NP (Nondeterministic Polynomial-Time)] NP is the set of all $R$ such that there exists TODO
\end{definition}

\begin{itemize}
\item NP-complete: if all other search problems reduce to it
\item NP-hard: 
\end{itemize}

\end{definition}
\clearpage

\section{Random Algorithm}
\begin{definition}[Expectation] (Has properties of linearity)
\[
\mathbb{E}[X] = \sum_{k=-\infty}^\infty k Pr(X=k)
\]
If $X$ is non-negative, then also
\[
\mathbb{E}[X]=\sum_{k=1}^\infty Pr(X \geq k)
\]
\end{definition}

\begin{definition}[Markov's Inequality]
If $X$ is non-negative, then
\[
Pr(X>\lambda) < \frac{\mathbb{E}[X]}{\lambda}
\]
\end{definition}

\subsection{Las Vegas}
\begin{definition}[Las Vegas]
Always correct. Runtime is a random variable that is small in expectation (e.g., guick sort).
\end{definition}

\begin{definition}[QuickSort] Randomly select a pivot point and recursively sort L and R. Define event $A_{ij} = $ $i$th smallest element had been compared to $j$th smallest element. Then
\begin{align*}
\mathbb{E}(runtime) \propto \sum_{i<j} \mathbb{E}[I_{A_{ij}}] &= \sum_{i<j} Pr(A_{ij}) \\
&= \sum_{i<j} \frac{2}{j-i+1} \\
&= \frac{1}{2}(n-1) + \frac{1}{3}(n-2)+\dots+\frac{1}{n}(1) \\
&\leq n\left(\frac{1}{2}+\frac{1}{3}+\dots+1\right) \\
&\leq n\ln n
\end{align*}
\end{definition}

\subsection{Monte Carlo}
\begin{definition}[Monte Carlo]
Always efficient. Correctness is a random variable (e.g., polling).
\end{definition}

\begin{definition}[Freivalds' Algorithm]
Algorithm for verifying matrix multiplication $C=AB$ is correct. Let $D=AB-C$, verify $Dx=0$.


If $D\neq 0$,
\begin{align*}
{Pr}_{x\in [0,1]^n}(Dx=0)\leq \frac{1}{2} \\
\implies Pr(incorrect) \leq \frac{1}{2^T}
\end{align*}
\end{definition}



\begin{definition}[Karger's Contraction Algorithm]
Used for solving the global min-cut problem. For weighted global min-cut, the problem can be reduced to $n-1$ calls to s-t cut. For unweighted global min-cut, Karger's algorithm performs $n-2$ random contractions and output the resulting cut.
\begin{align*}
Pr(\text{Karger returns min-cut}) \geq \frac{1}{\binom{n}{2}} \\
\implies Pr(incorrect) \leq \left(1-\frac{1}{\binom{n}{2}}\right)^T \leq e^{-\frac{T}{\binom{n}{2}}}
\end{align*}

\begin{remark}
No graph can have more than $\binom{n}{2}$ min-cuts.
\end{remark}
\end{definition}
\clearpage

\section{Hashing}
\begin{definition}[Hashing]
\[
\mathbb{E}[\text{op time}] = O\left(T_h + \tfrac{n}{m}\right)
\]
\end{definition}

\begin{definition}[K-wise independent hash families]
A set $H$ of functions mapping $\{ 0,\dots, U-1 \}$ into $\{ 0,\dots, m-1 \}$ is k-wise independent if
\[
\forall x_1\neq x_2\neq\dots\neq x_u, \forall y_1\dots y_k, {Pr}_{h\in H}\left( (h(x_1)=y_1) \wedge \dots \wedge (h(x_u)=y_u)      \right) = \frac{1}{m^k}
\]
\end{definition}

\begin{definition}[Universal hash families]
A set $H$ is universal if
\[
\forall x_i \neq x_j, {Pr}_{h\in H} (h(x_i)=h(x_j)) = \frac{1}{m}
\]
\end{definition}

\begin{remark}
2-wise independent $\implies$ universal
\end{remark}
\end{document}


