\documentclass[11pt]{article}

\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate,amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{threeparttable, adjustbox, booktabs}

\def\endproofmark{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\renewcommand\arraystretch{1.5}

%-----------------------------------------------------------------------------------

% Title information
\title{Efficient Algorithms and Intractable Problems}
\author{Daniel Deng}
\pagestyle{myheadings}
\date{}

%-----------------------------------------------------------------------------------

\begin{document}
\maketitle

\section{Complexity Analysis}
\begin{definition}[Partial Sums]
\begin{align*}
S_k = \sum_{n=1}^k a_n = \frac{k}{2}(a_1+a_k) && \text{(Arithmetic Series)} \\
S_k = \sum_{n=1}^k a_1(r)^n = a_1 \left(\frac{1-r^k}{1-r}\right) && \text{(Geometric Series)} \\
\end{align*}
\end{definition}


\begin{definition}[Asymptotic Notations]
\begin{align*}
f = O(g) \approx f(n) \leq c\cdot g(n) \\
f = o(g) \approx f(n) < c\cdot g(n) \\
f = \Omega(g) \approx f(n) \geq c\cdot g(n) \\
f = \omega(g) \approx f(n) > c\cdot g(n) \\
f = \Theta(g) \approx f(n) = c\cdot g(n) \\
\end{align*}
\end{definition}

\begin{remark}
$O(i^n \mid i > 1) > O(n^j) > O(log^k n)$
\end{remark}

\begin{theorem}[Master Theorem]
If $T(n)=aT([n/b])+O(n^d)$ for some constants $a>0$, $b>1$, and $d\geq 0$, then
\begin{equation*}
T(n)=\begin{cases}
\Theta(n^d) &a<b^d \\
\Theta(n^d \log n) & a = b^d \\
\Theta(n^{\log_b a}) & a > b^d \\
\end{cases}
\end{equation*}
\end{theorem}

\section{Polynomial Interpolation}
Given a degree $n$ polynomial $A(x)=a_0 + a_1 x + a_2 x^2+\cdots + a_{n} x^n$, the relationship between its values and coefficients can be represented by
$$
\begin{bmatrix}
A(x_0) \\ A(x_1) \\ \vdots \\ A(x_{n-1}) \\
\end{bmatrix} = \begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^{n-1} \\
1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
&& \vdots \\
1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-1} \\
\end{bmatrix} \begin{bmatrix}
a_0 \\ a_1 \\ \vdots \\ a_{n-1} \\
\end{bmatrix} \quad (\mbox{evaluation})
$$
where the matrix $M$ is a \textit{Vandermonde} matrix.

\subsection{Fast Fourier Transform (FFT)}
\begin{definition}[Discrete Fourier Transform Matrix]
For polynomials of degree $<n$ ($n$ is even; polynomials can be 0-padded), the Discrete Fourier Transform can be represented by the matrix
$$M_n (\omega) = \begin{bmatrix}
1 & 1 & 1 & \cdots & 1 \\
1 & \omega & \omega^2 & \cdots & \omega^{n-1} \\
&&\vdots \\
1 & \omega^j & \omega^{2j} & \cdots & \omega^{(n-1)j} \\
&&\vdots \\
1 & \omega^{n-1} & \omega^{2(n-1)} & \cdots & \omega^{(n-1)(n-1)} \\
\end{bmatrix}$$ \\
where $\omega = e^{2\pi i / n}$ is the $n$th root of unity.
\end{definition}

\begin{remark}
$M_n (\omega)$ is an unitary matrix whose columns forms the \textit{Fourier Basis}.
\end{remark}

\begin{lemma}
$M_n^{-1} (\omega) = \frac{1}{n} \overline{M_n (\omega)} = \frac{1}{n} M_n (\omega^{-1})$
\end{lemma}

\begin{lemma} 
$A(x)=A_{even}(x^2)+xA_{odd}(x^2)$ 
\end{lemma}

\begin{remark}
If $A$ is evaluated at points $\pm \omega_0,\dots, \pm \omega_{n/2-1}$, then $A_e(x^2)$ and $A_o(x^2)$ will only need to evaluate half the amount of points ($T(n) = 2T(n/2) + O(n) = O(n \log n)$).
\end{remark}

\begin{algorithm}[ht]
    \caption{Fast Fourier transform}
	\SetKwFunction{FFT}{FFT}  
	\SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
	\KwIn{A coefficient vector, $\vec{a}=\langle a_0,\dots,a_{n-1} \rangle$ and the $n$th root of unity, $\omega$.}
	\KwOut{$M_n(\omega)\vec{a}$}
    
    \Fn{\FFT{$\vec{a}$, $\omega$}}{
    \eIf{$\omega = 1$}{
    	return $\vec{a}$
    } {
    	$\langle A_e(0),\dots,A_{e}(n/2-1)\rangle \gets \mbox{FFT}(\langle a_0, a_2,\dots,a_{n-2}\rangle,\omega^2)$ \\
    	$\langle A_o(0),\dots,A_{o}(n/2-1)\rangle \gets \mbox{FFT}(\langle a_1, a_3,\dots,a_{n-1}\rangle,\omega^2)$ \\
    	\For {$j:=0$ to $n/2-1$}{
    		$A(j) \gets A_{e}(j) + \omega^j A_{o}(j)$ \\
    		$A(j+n/2) \gets A_{e}(j) - \omega^j A_o(j)$ \\
    	}
    }
    \Return {$\langle A(0), \dots, A(n-1)\rangle$}}
\end{algorithm}

\clearpage
\subsection{Applications of FFT}
\begin{algorithm}[ht]
    \caption{Fast Polynomial Multiplication}
    
    \DontPrintSemicolon
    \KwIn{Coefficient vectors, $a$ and $b$, and the $n$th root of unity, $\omega$.}
    \KwOut{The coefficient vector of $A(x)B(x)$}
    
    $\hat{\vec{a}} \gets M_n(\omega)\vec{a}$ \quad \mbox{(FFT)}\\
    $\hat{\vec{b}} \gets M_n(\omega)\vec{b}$\\
    \For {$i=0$ to $n-1$}{$\hat{c}_i\gets \hat{a}_i\hat{b}_i$}
    \Return {$\frac{1}{n} M_n(\omega^{-1})\hat{\vec{c}}$} \quad \mbox{(inverse matrix)}
\end{algorithm}

\begin{definition}[Cross-Correlation]
$corr(\vec{x},\vec{y})[k] = \sum x_i y_{i-k}$, which measures similarity.
\end{definition}

\begin{algorithm}
    \caption{Cross-Correlation}
 
    \DontPrintSemicolon
    \KwIn{Two signal vectors, $\vec{x}$ and $\vec{y}$.}
    \KwOut{$corr(\vec{x},\vec{y})$}
    
    $X(t) \gets x_{m-1} + x_{m-2}t + \dots + x_0 t^{m-1}$ \\
    $Y(t) \gets y_0 + y_1 t + \dots + y_{n-1} t^{n-1}$ \\
    $Q(t) \gets X(t)Y(t)$ \quad \mbox{(Fast Polynomial Multiplication)}\\
    \Return {$\vec{q}$}
\end{algorithm}

\section{Graphs}
\begin{definition}[Graph] A graph is a pair $G=(V,E)$, typically represented by an adjacency matrix or an adjacency list.
\end{definition}

\begin{table}[ht]
\centering
\caption{Graph representations.}
\begin{tabular}[t]{lcccc}
\hline
& Space & Connectivity & getNeighbors($u$) & DFS Runtime\\
\hline
Adjacency Matrix & $\Theta(|V|^2)$ & $O(1)$ & $\Theta(|V|)$ & $\Theta(|V|^2)$\\
Adjacency List & $\Theta(|V|+|E|)$ & $\Theta(degree(u))$ & $\Theta(degree(u))$ & $\Theta(|V|+|E|)$ \\
\hline
\end{tabular}
\end{table}

\newpage
\subsection{Depth-First Search}
\begin{algorithm}[ht]
    \caption{Depth-first search}
    \SetKwFunction{DFS}{DFS}
    \SetKwFunction{EXPLORE}{EXPLORE}
    \SetKwProg{Fn}{Function}{:}{}  
 
    \DontPrintSemicolon
    
	\KwIn{$V, E$ of directed graph $G$.}   
    
	\Fn{\DFS{$V,E$}}{
		$n \gets |V|$ \\
		$clk \gets 1$ \\
    	visited $\gets$ boolean[$n$] \\
    	preorder, postorder = int[$n$] \\
    	\For {$v \in V$} {
    		\If {!visited[v]} {
    			\EXPLORE{v}{}
    		}
    	}
	}
	\Fn{\EXPLORE{$v$}} {
		visited[v] $\gets$ True \\
		preorder[v] $\gets$ clk++ \\
		\For {$(v,w)\in E$}{
			\If{!visited[w]} {
				\EXPLORE{w}
			}		
		}
		postorder[v] $\gets$ clk++ \\
	}
	\tcc{Preorder-postorder intervals are either nested or disjoint.
}
	\tcc{postorder[$u$] $\leq$ postorder[$v$] \textit{iff} $(u,v)$ is a back edge.
}
	\tcc{$G$ contains a cycle \textit{iff} it contains a back edge.
}
\end{algorithm}

\begin{algorithm}[ht]
    \caption{Topological sort.}
    
    \DontPrintSemicolon
    \KwIn{A directed cyclic graph $G$.}
    \KwOut{An ordered list of $V$ such that $u_i$ comes before $v_i$ for all $(u_i, v_i)\in E$ (i.e., ordered by decreasing dependency).}
    
    $post \gets$ DFS-visited vertexes ordered by postorder visits \\
    \Return{reverse(post)}
\end{algorithm}

\begin{definition}[Strongly Connected Component]
A SCC is a maximal partition of a directed graph in which every vertex is reachable from every other vertex.
\end{definition}
\begin{align*}
u \text{ is in sink SCC of graph }G &\Leftrightarrow u \text{ is in source SCC of reverse graph }G \\ &\Leftrightarrow 
\text{u is in source SCC if highest postorder number.}
\end{align*}


\subsection{Single-Source Shortest Path}
\begin{algorithm}[ht]
    \caption{Single-Source Shortest Path}    
    
	\SetKwFunction{BFS}{BFS} 
	\SetKwFunction{Dijkstra}{Dijkstra} 
	\SetKwFunction{BF}{Bellman-Ford}
    \SetKwProg{Fn}{Function}{:}{}      
    
    \KwIn{A directed graph $G$ and a start vertex $S$.}
    \KwOut{Two arrays $prev[|V|]$ (shortest-path predecessor) and $dist[|V|]$ (shortest-path distance).}
    
    \Fn{\BFS{$G,S$}}{
	\tcc{Must have uniform edge weights. $O(|V|+|E|)$ runtime.}    
    }
    
    \Fn{\Dijkstra($G, S$)}{
    \tcc{Must have positive edge weights. $O(|V| \log |V| + |E|)$ runtime if implemented using Fibonacci heap.}
    }
    
    \Fn{\BF($G,S$)}{
    \tcc{Can have aribitrary edge weights.}
    }
\end{algorithm}

\subsection{Minimum Spanning Tree}
\begin{algorithm}[ht]
    \caption{Minimum spanning tree.}
    \SetKwFunction{P}{Prim}
    \SetKwFunction{K}{Kruskal}
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    \KwIn{A graph $G$ and a starting vertex $v$.}
    \KwOut{The minimum spanning tree $T$ of $G$.}
    \tcc{Use the cut property.}
    
    \Fn{\P{$G,v$}}{
    	\tcc{Sequentially adds the closest neightbor of the running set. $O(|E| + \log|V|)$ runtime if implemented using Fibonacci heap.}
    }
    
    \Fn{\K{$G,v$}}{
    	\tcc{Sequentially adds the shortest edge that does not create a cycle. $O(|E|\log|V|)$ runtime if implemented using Union-Find data structure.}
    }
\end{algorithm}

\section{Greedy Algorithm}
\begin{definition}[Greedy Algorithm]
A greedy algorithm is one that builds the solution iteratively using a sequence of local choices.
\end{definition}

\begin{algorithm}
    \caption{Example greedy algorithms.}
    \SetKwFunction{Scheduling}{SCHEDULING}  
    \SetKwFunction{Huffman}{HUFFMAN}
    \SetKwFunction{Set}{SET-COVER}
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    
    \Fn{\Scheduling} {
	\tcc{Find the maximum set of jobs that can be completed within time by iteratively select the next job to have the smallest end time without conflicting existing schedule. $O(n \log n)$ runtime if sorting the collection of jobs first.}    
    }
    
    \Fn{\Huffman}{
    \tcc{Find a prefix tree for prefix-free Huffman coding by iteratively combine the two least frequent elements of the alphabet and retrieve the order of the prefix tree accordingly. $O(n \log n)$ runtime if implemented with min-heap.}
    }
    
    \KwIn{A set of partitions $S=\{S_1,\dots,S_m\}$ that covers the universe $\{1,\dots,n\}$.}
    \KwOut{The indices of the smallest sub-collection of $S$ that covers the universe.}
	\Fn{\Set}{	
	\tcc{Greedy search yields sub-optimal but competitive solution to the set-cover problem. If the optimal solution uses $k$ sets, then the greedy solution uses at most $k \ln n$ sets.}    
    $A \gets \{1,\dots,n\}$ \\
    $B \gets \emptyset$ \\
    \While{$|A|>0$}{
    	let $i\in [m]\setminus B$ be s.t. $|A \cap S_i|$ is maximum \\
    	$A \gets A \setminus S_i$ \\
    	$B \gets B \cup {i}$
    }   \Return{$B$}    }
\end{algorithm}

\section{Union-Find}
\begin{definition}[Amortized Analysis]
Suppose a data structure supports $k$ operations. Then the amortized cost of each operation is $t_i$ if for any sequence of operations with $N_i$ of $O_i$ operations, the total time is at most $\sum_{i=1}^k t_i N_i$.
\end{definition}

\begin{algorithm}
    \caption{Union-find (disjoint forest implementation).}
    \SetKwFunction{Make}{MAKE-SET}
    \SetKwFunction{Find}{FIND} 
    \SetKwFunction{Union}{UNION} 
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    
	\tcc{$O((m+n)\log^* n)$ runtime.}    
    
    $parent[1,\dots,n]$\\
    $rank[1,\dots,n]$     \tcp{rank is defined as the height if no path compression occur.}
    \Fn{\Make{$x$}}{
	$parent[x] \gets x$ \\
	$rank[x] \gets 0$ \\    
    }
    
    \Fn{\Find{$x$}} {
	\If{$x = parent[x]$} {
	\Return{$x$}
	}
	$parent[x] \gets$ \Find{$parent[x]$} \tcp{path compression}
    \Return{$parent[x]$}
    }
    
    \Fn{\Union{$x,y$}} {
    	$x\gets$ \Find{$x$} \\
    	$y\gets$ \Find{$y$} \\
    	\If{$x=y$} {
    		\Return \tcp{no work needed}
    	}
    	\If{$rank[x]>rank[y]$}{
			swap $x$ and $y$    	
    	}
    	$parent[x] \gets y$
    	\If{$rank[x] = rank[y]$} {
			$rank[y] \gets rank[y] + 1$    	
    	}
    }
\end{algorithm}
\begin{remark}
Union-Find Invariants:
\begin{itemize}
\item a tree rooted at $x$ has $\geq 2^{r[x]}$ items;
\item $(\forall x)$, if $x$ is not a root, $r[p[x]] > r[x]$;
\item the number of items of exactly rank $k$ is $\leq \frac{n}{2^k}$.
\end{itemize}
\end{remark}

\section{Dynamic Programming}
\begin{definition}[Top-Down DP/Memoization]
Recursion + look-up table
\end{definition}

\begin{definition}[Bottom-Up DP]
Fill up the look-up table iteratively instead of recursively
\end{definition}

\begin{remark}
Bottom-up DP sometimes have better memory
\end{remark}

\section{Linear Programming}
\begin{definition}[Linear Programming]
Linear programming describes a broad class of optimization tasks in which both the constraints and the optimization criterion are linear functions.
\end{definition}

\begin{remark}
The optimum of a linear program is achieved at a vertex of the feasible region.
\end{remark}

\begin{remark}
A linear program does not have an optimum \textit{iff} it is infeasible or unbounded.
\end{remark}

\begin{definition}[Duality] Given two forms of the same LP:
\begin{itemize}
\item Primal LP --- $\underset{\vec{x}}{\text{argmax}}\, \{\vec{c}^{\,\textsf{T}} \vec{x} \mid A\vec{x}\leq \vec{b} \}$
\item Qual LP --- $\underset{\vec{y}}{\text{argmin}}\, \{\vec{b}^{\,\textsf{T}} \vec{y} \mid A^\textsf{T}\vec{y} = \vec{c}, \, \vec{y} \geq 0 \}$
\end{itemize}
Then, 
\begin{itemize}
\item Weak duality --- $\text{OPT}(P) \leq \text{OPT}(D)$
\item Strong duality --- Primal LP is bounded and feasible $\implies$ Dual LP is bounded and feasible $\wedge$ $\text{OPT}(P) = \text{OPT}(D)$
\end{itemize}
\end{definition}
\begin{remark}
Dual/Primal unbounded $\implies$ Primal/Dual infeasible.
\end{remark}
\subsection{LP Algorithm Overview}
\begin{definition}[Simplex]
A greedy algorithm that starts at a vexter and keeps moving to better vertices
\end{definition}
TODO

\section{Network Flow}

\end{document}


