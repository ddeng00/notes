\documentclass[11pt]{article}

\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate,amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{threeparttable, adjustbox, booktabs}

\def\endproofmark{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\renewcommand\arraystretch{1.5}

%-----------------------------------------------------------------------------------

\title{Introduction to Artificial Intelligence}
\author{Daniel Deng}
\pagestyle{myheadings}
\date{}

%-----------------------------------------------------------------------------------

\begin{document}
\maketitle

\section{Search Problems}
\begin{definition}[Reflex Agent]
A reflex agent chooses actions based on its current perception of the world.
\end{definition}

\begin{definition}[Planning Agent]
A planning agent chooses actions based on hypothesized consequences of actions.
\end{definition}

\begin{definition}[Search Problem]
A search problem consists of a state space, a successor function, a start state, and a goal test.
\end{definition}

\section{Search Algorithms}

\subsection{Heuristics}
\begin{definition}[Heuristic] 
A heuristic $h(n)$ is a function that estimates the distance from state $n$ to the goal state for a particular search problem. It is often solutions of relaxed problems.
\end{definition}

\begin{definition}[Admissibility] 
A heuristic is admissible, or optimistic, if $0 \leq h(n) \leq h^*(n)$ where $h^*$ is the true cost to goal state.
\end{definition}

\begin{definition}[Consistency]
A heuristic is consistent if $h(n) - h(n+1) \leq c(n, n+1)$ where $c$ is the cost between states $n$ and $n+1$.
\end{definition}

\begin{remark}
Consistency necessarily implies admissibility.
\end{remark}

\clearpage
\begin{table}[ht]
\centering
\begin{adjustbox}{width={\textwidth}}
\begin{threeparttable}
\caption{Search algorithms.}
\begin{tabular}[t]{lccccc}
\hline
& Fringe & Complete & Optimal & Time & Space \\ 
\hline
Depth-First Search & Stack & \textit{iff} no cycle & No & $O(b^m)$ & $O(bm)$ \\ 
Breadth-First Search & Queue & Yes & \textit{iff} uniform cost & $O(b^s)$\tnote{1} & $O(b^s)$\tnote{1} \\ 
Uniform Cost Search & PQ ($g(n)$)\tnote{2} & \textit{iff} positive cost & Yes & $O(b^{c^*/\epsilon})$\tnote{3} & $O(b^{c^*/\epsilon})$\tnote{3} \\ 
Greedy Search & PQ ($h(n)$)& - & No & - & - \\ 
$A^*$ Tree Search & PQ ($h(n)+g(n)$)& - & \textit{iff} $h(n)$ admissible & - & - \\ 
$A^*$ Graph Search\tnote{4} & PQ ($h(n)+g(n)$) & - & \textit{iff} $h(n)$ consistent & - & - \\ 
\hline
\end{tabular}
\quad
\begin{tablenotes}\footnotesize
\item[1] $s$ = depth of solution.
\item[2] $g(n)$ = cumulative path cost.
\item[3] $c^*/\epsilon$ = effective solution depth ($c^*$ = cost of the cheapest solution; $\epsilon$ = minimum cost of cost-contour arcs).
\item[4] Compared to tree search, graph search keeps a closed set of expanded states to check against to prevent duplicate expansions.
\end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{table}

\begin{remark}
Implementation of search algorithms differ only in fringe strategies.
\end{remark}

\newpage
\section{Constrained Satisfaction Problems}
\begin{definition}[Constrained Satisfaction Problems]
Constrained Satisfaction Problems (CSPs) are a type of \textbf{identification problem} defined by variable $X_0, \dots, X_n$ with values from a domain $D$ that satisfies a set of constrains.
\end{definition}

\begin{algorithm}[ht]
    \caption{Backtracking search.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{BS}{BS}
    \SetKwFunction{EX}{EXPLORE}   
    \SetKwFunction{D}{DOMAIN}
    \SetKwFunction{V}{VARIABLES}
    \SetKwFunction{C}{CONSTRAINTS}
    
    \DontPrintSemicolon
    \KwIn{A constraint satisfaction problem $P$.}
    \KwOut{A complete assignment $A$.}
    
    \Fn{\BS{$P$}}{
		\Return{\EX($P$, $\{\}$)}
    }
    
    \Fn{\EX{$P$, $A$}} {
    	\If{$A$ is complete} {
    		\Return{$A$}
    	}
    	$unassigned \gets$ an unassigned \V{$P$} \\
    	\ForEach{$value \in$ \D{$unassigned$}} {
    		\If{$value$ is consistent with all \C{$P$}} {
    			add $\{unassigned \gets value\}$ to $A$\\
    			$attempt \gets$ \EX{$P$, $A$} \\
    			\If{$attemp$ failed}{
    				remove $\{unassigned \gets value\}$ from $A$\\
    			} \Else {
    				\Return{$attempt$}
    			}
    		}
    	}
    	\Return{failed}
    }
\end{algorithm}

\newpage
\subsection{Filtering}
\begin{definition}[Arc Consistency]
\begin{multline*}
\text{Arc } X \to Y \text{ is consistent} \Leftrightarrow \\(\forall x \in D_x)(\exists y \in D_y)(y\text{ can be assigned to \textit{Y} without violating a constraint.})
\end{multline*}
\end{definition}

\begin{algorithm}[ht]
    \caption{Arc consistency filtering.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{AC}{AC-3}
    \SetKwFunction{F}{FILTER} 
    \SetKwFunction{N}{NEIGHBORS}
    \SetKwFunction{DOM}{DOMAIN}
    
    \DontPrintSemicolon
    \KwIn{A constraint satisfaction problem $P$.}
    
    \Fn{\AC{$P$}}{
		$Q \gets$ empty Queue \\
		enqueue all arcs $\in P$\\
		\While{$Q$ is not empty} {
			$(X_i, X_j) \gets$ dequeue from $Q$ \\
			\If{\F{$X_i, X_j$} is successful} {
				\ForEach{$X_k \in$ \N{$X_i$}} {
					enqueue $(X_k, X_i)$
				}
			}
		}
    }
    \Fn{\F($tail, head$)}{
    	$result \gets$ false \\
    	\ForEach{$value \in$ \DOM{$tail$}}{
    		\If{$value$ violates some constraint with all values in \DOM{$head$}} {
    		delete $value$ from \DOM{$tail$} \\
    		$result \gets$ true
    		}
    	}
    	\Return{$reuslt$}
    }
\end{algorithm}

\subsection{Ordering}
\begin{definition}[Minimum Remaining Values]
The MRV policy chooses an unassigned variable that has the fewest valid remaining values in order to induce backtracking earlier and reduce potential node expansions.
\end{definition}

\begin{definition}[Least Constraining Value]
The LCV policy chooses a value assignment that violates the least amount of constraints, which requires additional computation such as running arc consistency test on each value.
\end{definition}

\subsection{Structure}
Given a tree-structured CSP, represent it as a directed acyclic graph. Enforcing arc consistency in reverse topological order then assigning in topological order ensures a runtime of $O(nd^2)$  (as opposed to $O(d^n)$ in the general case). \\
TODO: nearly tree-like CSPs and tree decomposition.
\section{Local Search}
Improve a single option until no further improvements can be made.
\begin{remark}
Generally, local search is faster and more memory efficient at the expense of completeness and optimality.
\end{remark}
\subsection{Iterative Algorithm for CSP/Hill Climbing}
Starting with a "complete" state, randomly select any conflicted variables and reassign values using min-conflicts heuristics.
\begin{remark}
Efficiency of the algorithm depends on $R=\tfrac{\text{number of constraints}}{\text{number of variables}}$; computation time is approximately constant time except when $R$ approaches the \textit{critical ratio}.
\end{remark}

\subsection{Simulated Annealing}
\begin{algorithm}
    \caption{Simulated annealing.}
    
    \DontPrintSemicolon
    \KwIn{A problem $P$ and a schedule/mapping from time to "temperature" $T$.}
    \KwOut{A solution state.}
    
    \tcc{Escape local maxima by allowing downhill movement based on a "temperature"-dependent probabilistic function.}
    
    $current \gets$ initial state of $P$ \\
    \For{$t \gets 1$ to $\infty$}{
		$temp \gets T[t]$ \\
		\If{$temp = 0$}{\Return{$current$}}
		\Else{
			$next \gets$ a randomly selected successor of $current$ \\
			$\Delta \gets$ \textsc{Value}[$next$] - \textsc{Value}[$current$] \\
			\If{$\Delta > 0$} {
				$current \gets next$			
			} \Else{
				$currnet \gets next$ with probability $e^{\frac{\Delta}{temp}}$
			}
		}
    }
\end{algorithm}

\section{Genetic Algorithms}
Keep best $N$ hypotheses at each step (selection) based on a fitness function and have pairwise crossover operations (and, optionally, mutation operations) to generate a new set of hypotheses.


\section{Games}
\begin{algorithm}
    \caption{Minimax with alpha-beta pruning.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{V}{VALUE}
    \SetKwFunction{Max}{MAX-VALUE}   
    \SetKwFunction{Min}{MIN-VALUE}
    \SetKwFunction{Exp}{EXP-VALUE}
    \SetKwFunction{MaxF}{MAX}
    \SetKwFunction{MinF}{MIN}
    
    \DontPrintSemicolon
    \KwIn{A game state $S$}
    \KwOut{The root minimax value.}
    
    \tcc{initialize $\alpha \gets -\infty$ and $\beta \gets \infty$}
    \Fn{\V{$S, \alpha, \beta$}}{
		\If{$S$ is a terminal state} {
			\Return{knwon temrinal value}
		}
		\If{the agent is maximizing} {
			\Return{\Max{$S, \alpha, \beta$}}
		} \If{the agent is minimizing} {
			\Return{\Min($S, \alpha, \beta$)}
		}
    }
    
    \Fn{\Max{$S, \alpha, \beta$}} {
		$v \gets -\infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MaxF{$v$, \V{$S'$}} \\
			\If{$v \geq \beta$}{
				\Return{$v$} \\
			}
			$\alpha \gets$ \MaxF{$\alpha, v$}
		}    \Return{$v$}
    }
    
    \Fn{\Min{$S, \alpha, \beta$}} {
		$v \gets \infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MinF{$v$, \V{$S'$}} \\
			\If{$v \leq \alpha$}{
				\Return{$v$} \\
			}
			$\beta \gets$ \MinF{$\beta, v$}
		} \Return{$v$}
    }
\end{algorithm}

\begin{algorithm} [ht]
    \caption{Expectimax.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{V}{VALUE}
    \SetKwFunction{Max}{MAX-VALUE}   
    \SetKwFunction{Min}{MIN-VALUE}
    \SetKwFunction{Exp}{EXP-VALUE}
    \SetKwFunction{MaxF}{MAX}
    \SetKwFunction{MinF}{MIN}
    \SetKwFunction{E}{E}
    
    \DontPrintSemicolon
    \KwIn{A game state $S$}
    \KwOut{The root minimax value.}
    
    \Fn{\V{$S$}}{
		\If{$S$ is a terminal state} {
			\Return{knwon temrinal value}
		}
		\If{the agent is maximizing} {
			\Return{\Max{$S$}}
		}  \If{the agent is randomizing} {
			\Return{\Exp{$S$}}
		}
    }
    
    \Fn{\Max{$S$}} {
		$v \gets -\infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MaxF{$v$, \V{$S'$}} \\
		}    \Return{$v$}
    }
    
    \Fn{\Exp{$S$}} {
    	$v \gets 0$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets \mathbb{E}\,[$\V{$S'$}$]$ \tcp{expected value}
		}    \Return{$v$}
    }
\end{algorithm}

\begin{remark}
Typically, performing Minimax and Expectimax to terminal states are too costly, which can be solved by terminating early and estimating the state utility with evaluation functions.
\end{remark}

\section{Markov Decision Processes}
\begin{remark}
Finite horizons (finite timestep before an agent terminates) and/or discount factors $(\gamma)$ ensure an agent terminates in MDP.
\end{remark}

\begin{definition}[Transition Function]
$T(s,a,s')=P(s' \mid s,a)$
\end{definition}

\begin{definition}[Value Iteration]
Initialize $V_0(s) \gets 0$ for all $s \in S$. Compute $\forall s \in S$
\begin{align*}
V_{k+1}(s) \gets \underset{a}{\max} \, \underset{s'}{\sum} \, T(s,a,s')[R(s,a,s')+\gamma V_k(s')] && \text{(value iteration)} \\
\end{align*}
until $V_i(s)$ convereges for all $s \in S$ (yields $V^*$). Then, compute $\forall s \in S$
\begin{align*}
\pi^*(s) \gets \underset{a}{\text{argmax}} \, \underset{s'}{\sum}\, T(s,a,s')[R(s,a,s')+\gamma V_k(s')] && \text{(policy extraction)}
\end{align*}
\end{definition}

\begin{definition}[Policy Iteration]
Define an initial policy $\pi_0$ (can be arbitrary, but ideality close to the optimal policy). Then, iteratively solve $\forall s \in S$
\begin{align*}
V^{\pi_i}(s) = \underset{s'}{\sum} \, T(s,\pi_i(s),s')[R(s,\pi_i(s),s')+\gamma V^{\pi_i}(s')] && \text{(policy evaluation)} \\
\pi_{i+1}(s) \gets \underset{a}{\text{argmax}} \, \underset{s'}{\sum}\, T(s,a,s')[R(s,a,s')+\gamma V^{\pi_i}(s')] && \text{(policy improvement)}
\end{align*}
until $\pi(s)$ converges for all $s\in S$ (yields $pi^*$.
\end{definition}
\begin{remark}
Policy evaluation solves a system of $|S|$ liear equations.
\end{remark}
\begin{remark}
Policy iteration converges faster than value iteration.
\end{remark}

\end{document}


