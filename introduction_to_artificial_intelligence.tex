\documentclass[11pt]{article}

\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate,amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{threeparttable, adjustbox, booktabs}

\def\endproofmark{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newcommand{\indep}{\perp \!\!\! \perp}

\renewcommand\arraystretch{1.5}

%-----------------------------------------------------------------------------------

\title{Introduction to Artificial Intelligence}
\author{Daniel Deng}
\pagestyle{myheadings}
\date{}

%-----------------------------------------------------------------------------------

\begin{document}
\maketitle

\section{Agents}
\begin{definition}[Reflex Agent]
A reflex agent chooses actions based on its current perception of the world.
\end{definition}

\begin{definition}[Planning Agent]
A planning agent chooses actions based on hypothesized consequences of actions.
\end{definition}
\clearpage

\section{General Search Problems}
\subsection{Heuristics}
\begin{definition}[Heuristic] 
A heuristic $h(n)$ is a function that estimates the distance from state $n$ to the goal state for a particular search problem. It is often solutions of relaxed problems.
\begin{enumerate}
\item A heuristic is \underline{admissible} if $0 \leq h(n) \leq h^*(n)$ where $h^*$ is the true cost to goal state;
\item A heuristic is \underline{consistent} if $h(n) - h(n+1) \leq c(n, n+1)$ where $c$ is the cost between states $n$ and $n+1$.
\end{enumerate}
\end{definition}
\begin{remark}
Consistency necessarily implies admissibility.
\end{remark}

\subsection{Search Algorithms}
\begin{table}[ht]
\centering
\begin{adjustbox}{width={\textwidth}}
\begin{threeparttable}
\caption{Search algorithms.}
\begin{tabular}[t]{lccccc}
\hline
& Fringe & Complete & Optimal & Time & Space \\ 
\hline
Depth-First Search & Stack & \textit{iff} no cycle & No & $O(b^m)$ & $O(bm)$ \\ 
Breadth-First Search & Queue & Yes & \textit{iff} uniform cost & $O(b^s)$\tnote{1} & $O(b^s)$\tnote{1} \\ 
Uniform Cost Search & PQ ($g(n)$)\tnote{2} & \textit{iff} positive cost & Yes & $O(b^{c^*/\epsilon})$\tnote{3} & $O(b^{c^*/\epsilon})$\tnote{3} \\ 
Greedy Search & PQ ($h(n)$)& - & No & - & - \\ 
$A^*$ Tree Search & PQ ($h(n)+g(n)$)& - & \textit{iff} $h(n)$ admissible & - & - \\ 
$A^*$ Graph Search\tnote{4} & PQ ($h(n)+g(n)$) & - & \textit{iff} $h(n)$ consistent & - & - \\ 
\hline
\end{tabular}
\quad
\begin{tablenotes}\footnotesize
\item[1] $s$ = depth of solution.
\item[2] $g(n)$ = cumulative path cost.
\item[3] $c^*/\epsilon$ = effective solution depth ($c^*$ = cost of the cheapest solution; $\epsilon$ = minimum cost of cost-contour arcs).
\item[4] Compared to tree search, graph search keeps a closed set of expanded states to check against to prevent duplicate expansions.
\end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{table}

\begin{remark}
Implementation of search algorithms differ only in fringe strategies.
\end{remark}
\clearpage

\section{Constrained Satisfaction Problems}
\begin{definition}[Constrained Satisfaction Problems]
Constrained Satisfaction Problems (CSPs) are a type of \textbf{identification problem} defined by variable $X_0, \dots, X_n$ with values from a domain $D$ that satisfies a set of constrains.
\end{definition}

\begin{algorithm}[ht]
    \caption{Backtracking search.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{BS}{BS}
    \SetKwFunction{EX}{EXPLORE}   
    \SetKwFunction{D}{DOMAIN}
    \SetKwFunction{V}{VARIABLES}
    \SetKwFunction{C}{CONSTRAINTS}
    
    \DontPrintSemicolon
    \KwIn{A constraint satisfaction problem $P$.}
    \KwOut{A complete assignment $A$.}
    
    \Fn{\BS{$P$}}{
		\Return{\EX($P$, $\{\}$)}
    }
    
    \Fn{\EX{$P$, $A$}} {
    	\If{$A$ is complete} {
    		\Return{$A$}
    	}
    	$unassigned \gets$ an unassigned \V{$P$} \\
    	\ForEach{$value \in$ \D{$unassigned$}} {
    		\If{$value$ is consistent with all \C{$P$}} {
    			add $\{unassigned \gets value\}$ to $A$\\
    			$attempt \gets$ \EX{$P$, $A$} \\
    			\If{$attemp$ failed}{
    				remove $\{unassigned \gets value\}$ from $A$\\
    			} \Else {
    				\Return{$attempt$}
    			}
    		}
    	}
    	\Return{failed}
    }
\end{algorithm}
\clearpage

\subsection{Filtering}
\begin{definition}[Arc Consistency]
\begin{multline*}
\text{Arc } X \to Y \text{ is consistent} \Leftrightarrow \\(\forall x \in D_x)(\exists y \in D_y)(y\text{ can be assigned to \textit{Y} without violating a constraint.})
\end{multline*}
\end{definition}

\begin{algorithm}[ht]
    \caption{Arc consistency filtering.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{AC}{AC-3}
    \SetKwFunction{F}{FILTER} 
    \SetKwFunction{N}{NEIGHBORS}
    \SetKwFunction{DOM}{DOMAIN}
    
    \DontPrintSemicolon
    \KwIn{A constraint satisfaction problem $P$.}
    
    \tcc{$O(|S||A|^3)$ runtime}
    \Fn{\AC{$P$}}{
		$Q \gets$ empty Queue \\
		enqueue all arcs $\in P$\\
		\While{$Q$ is not empty} {
			$(X_i, X_j) \gets$ dequeue from $Q$ \\
			\If{\F{$X_i, X_j$} is successful} {
				\ForEach{$X_k \in$ \N{$X_i$}} {
					enqueue $(X_k, X_i)$
				}
			}
		}
    }
    \Fn{\F($tail, head$)}{
    	$result \gets$ false \\
    	\ForEach{$value \in$ \DOM{$tail$}}{
    		\If{$value$ violates some constraint with all values in \DOM{$head$}} {
    		delete $value$ from \DOM{$tail$} \\
    		$result \gets$ true
    		}
    	}
    	\Return{$reuslt$}
    }
\end{algorithm}

\subsection{Ordering}
\begin{definition}[Minimum Remaining Values]
The MRV policy chooses an unassigned variable that has the fewest valid remaining values in order to induce backtracking earlier and reduce potential node expansions.
\end{definition}

\begin{definition}[Least Constraining Value]
The LCV policy chooses a value assignment that violates the least amount of constraints, which requires additional computation such as running arc consistency test on each value.
\end{definition}

\subsection{Structure}
Given a tree-structured CSP, represent it as a directed acyclic graph. Enforcing arc consistency in reverse topological order then assigning in topological order ensures a runtime of $O(nd^2)$  (as opposed to $O(d^n)$ in the general case). \\
TODO: nearly tree-like CSPs and tree decomposition.
\clearpage

\section{Local Search}
\begin{definition}[Local Search]
A search strategy that improve a single option until no further improvements can be made. Typically, local search is faster and more memory efficient than other search algorithms but is generally neither complete nor optimal.
\end{definition}

\begin{definition}[Hill Climbing]
A CSP strategy that randomly selects a conflicting variable and reassign values using min-conflicts heuristics.
\end{definition}

\begin{remark}
Efficiency of the algorithm depends on $R=\tfrac{\text{number of constraints}}{\text{number of variables}}$; computation time is approximately constant time except when $R$ approaches the \textit{critical ratio}.
\end{remark}

\subsection{Simulated Annealing}
\begin{algorithm}[ht]
    \caption{Simulated annealing.}
    
    \DontPrintSemicolon
    \KwIn{A problem $P$ and a schedule/mapping from time to "temperature" $T$.}
    \KwOut{A solution state.}
    
    \tcc{Escape local maxima by allowing downhill movement based on a "temperature"-dependent probabilistic function.}
    
    $current \gets$ initial state of $P$ \\
    \For{$t \gets 1$ to $\infty$}{
		$temp \gets T[t]$ \\
		\If{$temp = 0$}{\Return{$current$}}
		\Else{
			$next \gets$ a randomly selected successor of $current$ \\
			$\Delta \gets$ \textsc{Value}[$next$] - \textsc{Value}[$current$] \\
			\If{$\Delta > 0$} {
				$current \gets next$			
			} \Else{
				$currnet \gets next$ with probability $e^{\frac{\Delta}{temp}}$
			}
		}
    }
\end{algorithm}

\subsection{Genetic Algorithm}
\begin{definition}[Genetic Algorithm]
A strategy that keeps the best $N$ hypothesis at each step based on a fitness function and generate next generation using pairwise cross-over operations (and, optionally, mutation operations).
\end{definition}
\clearpage

\section{Games}
\begin{definition}[Games]
Games are multi-agent search problems that could be zero-sum (adversarial) or general sum.
\end{definition}
\subsection{Minimax}
\begin{definition}[Minimax]
A zero-sum-game algorithm that assumes the opponent is an optimal adversary.
\end{definition}
\begin{algorithm}
    \caption{Minimax with alpha-beta pruning.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{V}{VALUE}
    \SetKwFunction{Max}{MAX-VALUE}   
    \SetKwFunction{Min}{MIN-VALUE}
    \SetKwFunction{Exp}{EXP-VALUE}
    \SetKwFunction{MaxF}{MAX}
    \SetKwFunction{MinF}{MIN}
    
    \DontPrintSemicolon
    \KwIn{A game state $S$}
    \KwOut{The root minimax value.}
    
    \tcc{initialize $\alpha \gets -\infty$ and $\beta \gets \infty$}
    \Fn{\V{$S, \alpha, \beta$}}{
		\If{$S$ is a terminal state} {
			\Return{knwon temrinal value}
		}
		\If{the agent is maximizing} {
			\Return{\Max{$S, \alpha, \beta$}}
		} \If{the agent is minimizing} {
			\Return{\Min($S, \alpha, \beta$)}
		}
    }
    
    \Fn{\Max{$S, \alpha, \beta$}} {
		$v \gets -\infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MaxF{$v$, \V{$S'$}} \\
			\If{$v \geq \beta$}{
				\Return{$v$} \\
			}
			$\alpha \gets$ \MaxF{$\alpha, v$}
		}    \Return{$v$}
    }
    
    \Fn{\Min{$S, \alpha, \beta$}} {
		$v \gets \infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MinF{$v$, \V{$S'$}} \\
			\If{$v \leq \alpha$}{
				\Return{$v$} \\
			}
			$\beta \gets$ \MinF{$\beta, v$}
		} \Return{$v$}
    }
\end{algorithm}

\subsection{Expectimax}
\begin{definition}[Expectimax]
A zero-sum-game algorithm that assumes the opponent acts based on some probability distribution.
\end{definition}
\begin{algorithm} [ht]
    \caption{Expectimax.}
    \SetKwProg{Fn}{Function}{:}{} 
    \SetKwFunction{V}{VALUE}
    \SetKwFunction{Max}{MAX-VALUE}   
    \SetKwFunction{Min}{MIN-VALUE}
    \SetKwFunction{Exp}{EXP-VALUE}
    \SetKwFunction{MaxF}{MAX}
    \SetKwFunction{MinF}{MIN}
    \SetKwFunction{E}{E}
    
    \DontPrintSemicolon
    \KwIn{A game state $S$}
    \KwOut{The root minimax value.}
    
    \Fn{\V{$S$}}{
		\If{$S$ is a terminal state} {
			\Return{knwon temrinal value}
		}
		\If{the agent is maximizing} {
			\Return{\Max{$S$}}
		}  \If{the agent is randomizing} {
			\Return{\Exp{$S$}}
		}
    }
    
    \Fn{\Max{$S$}} {
		$v \gets -\infty$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets$ \MaxF{$v$, \V{$S'$}} \\
		}    \Return{$v$}
    }
    
    \Fn{\Exp{$S$}} {
    	$v \gets 0$ \\
		\ForEach{successor $S'$ of $S$}{
			$v \gets \mathbb{E}\,[$\V{$S'$}$]$ \tcp{expected value}
		}    \Return{$v$}
    }
\end{algorithm}
\clearpage

\section{Markov Decision Processes}
\begin{remark}
Finite horizons (finite timestep before an agent terminates) and/or discount factors $(\gamma)$ ensure an agent terminates in MDP.
\end{remark}

\begin{definition}[Transition Function]
$T(s,a,s')=P(s' \mid s,a)$
\end{definition}

\subsection{Value Iteration}
\begin{algorithm}
    \caption{Value Iteration}
    \SetKwFunction{Policy}{POLICY-EXTRACTION}
    \SetKwFunction{Value}{VALUE-ITERATION}  
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    \KwIn{A MDP $(S,A,R,T,\gamma)$.}
    \KwOut{The optimal policy $\pi^*(s)$ for all $s\in S$.}
    
    \Fn{\Policy{}} {
    	$V^* \gets$ \Value{} \tcp{optimal value}
    	\ForEach{$s\in S$} {
    	$\pi^*(s) \gets \underset{a}{\text{argmax}} \, \underset{s'}{\sum}\, T(s,a,s')[R(s,a,s')+\gamma V_k(s')]$
    	}
    	\Return{$\pi^*(s) \mid \forall s \in S$}
    }
    \Fn{\Value{}} {\tcp{$O(|S|^2 |A|)$ runtime}
    	Initialize $V_0(s) \gets 0$ for all $s \in S$ \\
    	\While{$V_{k+1}(s) \neq V_k(s) \mid \forall s \in S$} {
    		\tcp{repeat until values converge}
    		$V_{k+1}(s) \gets \underset{a}{\max} \, \underset{s'}{\sum} \, T(s,a,s')[R(s,a,s')+\gamma V_k(s')]$ \\
    	}
    	\Return{$V^*(s) \gets V_{k+1}(s) \mid \forall s \in S$}
    }
\end{algorithm}

\subsection{Policy Iteration}
\begin{definition}[Policy Iteration]
Define an initial policy $\pi_0$ (can be arbitrary, but ideality close to the optimal policy). Then, iteratively solve $\forall s \in S$
\begin{align*}
V^{\pi_i}(s) = \underset{s'}{\sum} \, T(s,\pi_i(s),s')[R(s,\pi_i(s),s')+\gamma V^{\pi_i}(s')] && \text{(policy evaluation)} \\
\pi_{i+1}(s) \gets \underset{a}{\text{argmax}} \, \underset{s'}{\sum}\, T(s,a,s')[R(s,a,s')+\gamma V^{\pi_i}(s')] && \text{(policy improvement)}
\end{align*}
until $\pi(s)$ converges for all $s\in S$ (yields $pi^*$.
\end{definition}
\begin{remark}
Policy evaluation solves a system of $|S|$ liear equations.
\end{remark}
\begin{remark}
Policy iteration converges faster than value iteration.
\end{remark}

\section{Reinforcement Learning}
\begin{remark}
Reinforcement learning operates on MDP problems where the $T$ and $R$ functions are unknown.
\end{remark}

\subsection{Model-Based Learning}
\begin{definition}[Model-Based RL]
An algorithm that counts and normalizes sample outcomes $s'$ for each $s,a$ to construct $\hat{T}(s,a,s')$ and discovers each $\hat{R}(s,a,s')$ through exploration. The approximated MDP is then solved by value or policy iteration.
\end{definition}

\subsection{Model-Free Learning}
\begin{definition}[Direct Evaluation]
An algorithm that fixes some policy $\pi$ and empirically computes $V^\pi(s)$ for all $s \in S$ by averaging total sample utility for a given state.
\end{definition}
\begin{remark}
Direct evaluation wastes information about state connections and will take a long time to learn.
\end{remark}

\begin{definition}[Temporal Difference Learning]
$$V^\pi(s) \gets (1-\alpha)V^\pi(s) + \alpha [R(s,\pi(s),s')+\gamma V^\pi(s')]$$
\end{definition}

\begin{remark}
Can't extract policy.
\end{remark}

\begin{definition}[Q-Learning]
$$Q(s,a) \gets (1-\alpha)Q(s,a) + \alpha [R(s,a,s')+\gamma \, \underset{a'}{\max}\,Q(s',a')]$$
\end{definition}
\begin{remark}
Q-learning is an example of off-policy learning, in which the algorithm can learn the optimal policy even by taking sub-optimal or random actions.
\end{remark}

\begin{definition}[Approximate Q-Learning] Represent q-values as weighted sums of features $Q(s,a) := \vec{w} \cdot \vec{f}(s,a)$. The update rule for Q-Learning then becomes
\begin{align*}
\Delta \gets [R'(s,a,s')+\gamma \, \underset{a'}{\max}\,Q(s',a')]-Q(s,a) \\
w_i \gets w_i + \alpha \cdot \Delta \cdot f_i(s,a)
\end{align*}
\end{definition}

\begin{definition}[$\epsilon$-Greedy Policies]
Define some probability $0\leq \epsilon \leq 1$ to act randomly and explore. $\epsilon$ should be lowered over time to favor more exploitation as the learning becomes complete.
\end{definition}

\begin{definition}[Exploration Function]
\begin{align*}
Q(s,a) \gets (1-\alpha)Q(s,a) + \alpha [R(s,a,s')+\gamma \, \underset{a'}{\max}\,f(s',a')] \\
f(s,a) = Q(s,a)+\tfrac{\text{const.}}{\text{count}(Q(s,a))}
\end{align*}
\end{definition}

\begin{remark}
$f(s,a)$ shown here is only a common example where the ``bonus'' for exploration diminishes as q-states are explored more and more.
\end{remark}
\clearpage

\section{Bayesian Network}
\begin{definition}[Conditional Independence]
$\forall x,y,z : P(x|z,y)=P(x|z)$ (i.e., $x \indep y \mid z$)
\end{definition}

\begin{definition}[Bayes' Net] Bayes' Net is a graphic model (DAG) that describes complex joint distributions using simple, local distributions (conditional probabilities)
\begin{itemize}
\item Nodes---variables (with domains) [assigned = observed, unassigned = unobserved]
\item Arcs---interactions (i.e., encode conditional independence when lacking arrows) $\implies P(x_i|x_1,\dots,x_{i-1})=P(x_i|parents(X_i))$
\item A conditional distribution for each node
\end{itemize}
\end{definition}
\begin{remark}
Bayes' Net does not imply causation, only the lack thereof
\end{remark}

\subsection{Inference}
\begin{definition}[Inference by Enumeration]
General procedure:
\begin{enumerate}
\item Join Factors: $\forall r,t: P(r,t)=P(r)P(t|r)$
\item Eliminate: $P(T)=\sum_r P(r,T)$
\item Normalize: $P(Q|e_1,\dots,e_k)=\frac{P(Q,e_1,\dots,e_k)}{\sum_q P(Q,e_1,\dots,e_k)}$
\end{enumerate}
\end{definition}

\begin{remark}
Variable elimination is faster than inference by enumeration via interleaving joining and marginalizing.
\end{remark}

\subsection{Sampling}
\begin{algorithm}
    \caption{Sampling.}
    \SetKwFunction{Prior}{PRIOR-SAMPLING} 
    \SetKwFunction{Rej}{REJECTION-SAMPLING} 
    \SetKwFunction{Like}{LIKELIHOOD-WEIGHTING-SAMPLING}
    \SetKwFunction{Gibbs}{GIBBS-SAMPLING}
    \SetKwProg{Fn}{Function}{:}{}  
    
    \DontPrintSemicolon
    
    \Fn{\Prior}{
		\For{$i=1,2,\dots,n$ in topological order} {
			Sample $x_i$ from $P(X_i \mid \text{Parents}(X_i))$
		} \Return{$(x_1, x_2,\dots,x_n)$}
    }
    \Fn{\Rej}{
    	\tcp{Will reject lots of samples if evidence is unlikely.}
    	\For{$i=1,2,\dots,n$ in topological order} {
			Sample $x_i$ from $P(X_i \mid \text{Parents}(X_i))$\\
			\If{$x_i$ inconsistent with evidence} {
				\Return \tcp{no sample generated}			
			}
		} \Return{$(x_1, x_2,\dots,x_n)$}
    }
    \Fn{\Like}{
		$w \gets 1.0$ \\    	
    	\For{$i=1,2,\dots,n$ in topological order} {
			\If{$X_i$ is an evidence variable}{
				$X_i \gets$ observation $x_i$ from $X_i$ \\
				$w \gets w \times P(x_i \mid \text{Parents}(X_i))$\\
			} \Else {
				Sample $x_i$ from $P(X_i \mid \text{Parents}(X_i))$\\
			}
		} \Return{$(x_1, x_2,\dots,x_n),w$}
    }
    \Fn{\Gibbs}{
    
    
    
    }
\end{algorithm}

\subsection{D-Separation}
\begin{definition}[Causal Chains] $X \to Y \to Z$ 
$$P(x,y,z) = P(x)P(y \mid x)P(z \mid y)\implies \neg(X \indep Z)  \wedge  (X\indep Z \mid Y)$$
\end{definition}

\begin{definition}[Common Causes] $X \leftarrow Y \to Z$
\[   P(x,y,z,) = P(y)P(x \mid y) P(z \mid y) \implies \neg(X \indep Z)  \wedge  (X\indep Z \mid Y)       \]
\end{definition}

\begin{definition}[Common Effect] $X\to Z \leftarrow Y$
\[  (X \indep Z)  \wedge    \neg (X\indep Z \mid Y)   \]
\end{definition}
\begin{definition}[D-Separation]
$X \indep Y \mid Z$ \textit{iff} $X$ and $Y$ are ``d-separated'' by $Z$ (i.e., all undirected path from $X$ to $Z$ are inactive). A path is active if all triple it contains is active:
\begin{itemize}
\item Causal chain $A\to B\to C$ where $B$ is unobserved
\item Common cause $A\leftarrow B \to C$ where $B$ is unobserved
\item Common effect $A \to B \leftarrow C$ where $B$ or one of its descendants is observed
\end{itemize}
\end{definition}


\end{document}


