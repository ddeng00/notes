\documentclass[11pt]{article}

\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate,amsthm}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{threeparttable, adjustbox, booktabs}

\def\endproofmark{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\renewcommand\arraystretch{1.5}

%-----------------------------------------------------------------------------------

\title{Introduction to Artificial Intelligence}
\author{Daniel Deng}
\pagestyle{myheadings}
\date{}

%-----------------------------------------------------------------------------------

\begin{document}
\maketitle

\section{Search Problems}
\begin{definition}[Reflex Agent]
A reflex agent chooses actions based on its current perception of the world.
\end{definition}

\begin{definition}[Planning Agent]
A planning agent chooses actions based on hypothesized consequences of actions.
\end{definition}

\begin{definition}[Search Problem]
A search problem consists of a state space, a successor function, a start state, and a goal test.
\end{definition}

\section{Search Algorithms}

\subsection{Heuristics}
\begin{definition}[Heuristic] 
A heuristic $h(n)$ is a function that estimates the distance from state $n$ to the goal state for a particular search problem. It is often solutions of relaxed problems.
\end{definition}

\begin{definition}[Admissibility] 
A heuristic is admissible, or optimistic, if $0 \leq h(n) \leq h^*(n)$ where $h^*$ is the true cost to goal state.
\end{definition}

\begin{definition}[Consistency]
A heuristic is consistent if $h(n) - h(n+1) \leq c(n, n+1)$ where $c$ is the cost between states $n$ and $n+1$.
\end{definition}

\begin{remark}
Consistency necessarily implies admissibility.
\end{remark}

\clearpage
\begin{table}[ht]
\centering
\begin{adjustbox}{width={\textwidth}}
\begin{threeparttable}
\caption{Search algorithms.}
\begin{tabular}[t]{lccccc}
\hline
& Fringe & Complete & Optimal & Time & Space \\ 
\hline
Depth-First Search & Stack & \textit{iff} no cycle & No & $O(b^m)$ & $O(bm)$ \\ 
Breadth-First Search & Queue & Yes & \textit{iff} uniform cost & $O(b^s)$\tnote{1} & $O(b^s)$\tnote{1} \\ 
Uniform Cost Search & PQ ($g(n)$)\tnote{2} & \textit{iff} positive cost & Yes & $O(b^{c^*/\epsilon})$\tnote{3} & $O(b^{c^*/\epsilon})$\tnote{3} \\ 
Greedy Search & PQ ($h(n)$)& - & No & - & - \\ 
$A^*$ Tree Search & PQ ($h(n)+g(n)$)& - & \textit{iff} $h(n)$ admissible & - & - \\ 
$A^*$ Graph Search\tnote{4} & PQ ($h(n)+g(n)$) & - & \textit{iff} $h(n)$ consistent & - & - \\ 
\hline
\end{tabular}
\quad
\begin{tablenotes}\footnotesize
\item[1] $s$ = depth of solution.
\item[2] $g(n)$ = cumulative path cost.
\item[3] $c^*/\epsilon$ = effective solution depth ($c^*$ = cost of the cheapest solution; $\epsilon$ = minimum cost of cost-contour arcs).
\item[4] Compared to tree search, graph search keeps a closed set of expanded states to check against to prevent duplicate expansions.
\end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{table}

\begin{remark}
Implementation of search algorithms differ only in fringe strategies.
\end{remark}

\section{Constrained Satisfaction Problems}
\begin{definition}[Constrained Satisfaction Problems]
Constrained Satisfaction Problems (CSPs) are a type of \textbf{identification problem} defined by variable $X_0, \dots, X_n$ with values from a domain $D$ that satisfies a set of constrains.
\end{definition}

\subsection{Backtracking Search}
\subsubsection{Filtering}
\begin{definition}[Arc Consistency]
\begin{multline*}
\text{Arc } X \to Y \text{ is consistent} \Leftrightarrow \\(\forall x \in D_x)(\exists y \in D_y)(y\text{ can be assigned to \textit{Y} without violating a constraint.})
\end{multline*}
\end{definition}
\subsubsection{Ordering}
\begin{definition}[Minimum Remaining Values]
The MRV policy chooses an unassigned variable that has the fewest valid remaining values in order to induce backtracking earlier and reduce potential node expansions.
\end{definition}

\begin{definition}[Least Constraining Value]
The LCV policy chooses a value assignment that violates the least amount of constraints, which requires additional computation such as running arc consistency test on each value.
\end{definition}

\subsubsection{Structure}
Given a tree-structured CSP, represent it as a directed acyclic graph. Enforcing arc consistency in reverse topological order then assigning in topological order ensures a runtime of $O(nd^2)$  (as opposed to $O(d^n)$ in the general case). \\
TODO: nearly tree-like CSPs and tree decomposition.
\section{Local Search}
Improve a single option until no further improvements can be made.
\begin{remark}
Generally, local search is faster and more memory efficient at the expense of completeness and optimality.
\end{remark}
\subsection{Iterative Algorithm for CSP/Hill Climbing}
Starting with a "complete" state, randomly select any conflicted variables and reassign values using min-conflicts heuristics.
\begin{remark}
Efficiency of the algorithm depends on $R=\tfrac{\text{number of constraints}}{\text{number of variables}}$; computation time is approximately constant time except when $R$ approaches the \textit{critical ratio}.
\end{remark}

\subsection{Simulated Annealing}
\begin{algorithm}
    \caption{Simulated annealing.}
    
    \DontPrintSemicolon
    \KwIn{A problem $P$ and a schedule/mapping from time to "temperature" $T$.}
    \KwOut{A solution state.}
    
    \tcc{Escape local maxima by allowing downhill movement based on a "temperature"-dependent probabilistic function.}
    
    $current \gets$ initial state of $P$ \\
    \For{$t \gets 1$ to $\infty$}{
		$temp \gets T[t]$ \\
		\If{$temp = 0$}{\Return{$current$}}
		\Else{
			$next \gets$ a randomly selected successor of $current$ \\
			$\Delta \gets$ \textsc{Value}[$next$] - \textsc{Value}[$current$] \\
			\If{$\Delta > 0$} {
				$current \gets next$			
			} \Else{
				$currnet \gets next$ with probability $e^{\frac{\Delta}{temp}}$
			}
		}
    }
\end{algorithm}

\section{Genetic Algorithms}
Keep best $N$ hypotheses at each step (selection) based on a fitness function and have pairwise crossover operations (and, optionally, mutation operations) to generate a new set of hypotheses.


\section{General-Sum Game}





\end{document}


